import os
import requests
from bs4 import BeautifulSoup
import time
import random
import asyncio
import aiohttp
import re

def safe_folder_name(name):
    """ç§»é™¤ä¸å…è¨±çš„å­—å…ƒ"""
    return re.sub(r'[\\/:*?"<>|]', "_", name)

def get_total_pages(url):
    """å–å¾—ç¸½é æ•¸"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    lastpage = soup.select("table.ptt td")[-2].text  # å€’æ•¸ç¬¬äºŒæ ¼æ˜¯æœ€å¾Œé ç¢¼
    return int(lastpage)
def find_makedirs(url,save_folder_path):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://e-hentai.org/",
        "Accept": "text/html",
        "Accept-Language": "zh-TW"
    }
    os.makedirs(save_folder_path, exist_ok=True)

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    folder_name = soup.find('head').find('title').get_text()
    folder_name = safe_folder_name(folder_name)
    
    path=os.path.join(save_folder_path,folder_name)
    os.makedirs(path, exist_ok=True)
    return path
async def get_image_urls_from_page(session, page_url):
    headers = {
            "User-Agent": "Mozilla/5.0",
            "Referer": "https://e-hentai.org/",
            "Accept": "text/html",
            "Accept-Language": "zh-TW"
        }
    page=int(page_url.split('=')[-1])+1
    try:
        async with session.get(page_url, headers=headers) as response:
            if response.status != 200:
                print(f"é é¢ {page} è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status}")
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, "html.parser")
            a_href=soup.find('div',id="gdt").find_all('a')
            if not a_href:
                print(f"é é¢ {page} æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡é€£çµ")
                return []
            page_img_urls_list = []
            for link in a_href:
                img_url = link.get('href')
                page_img_urls_list.append(img_url)
            print(f"ç¬¬ {page} é å·²å®Œæˆè§£æ")
            return page_img_urls_list
    except Exception as e:
        print(f"è§£æé é¢ {page} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return []
async def download_image(session, img_url, path):
    headers = {
            "User-Agent": "Mozilla/5.0",
            "Referer": "https://e-hentai.org/",
            "Accept": "text/html",
            "Accept-Language": "zh-TW"
        }
    img_name=f"ç¬¬{img_url.split('-')[-1]}é .jpg"
    try:
        async with session.get(img_url, headers=headers) as response:
            if response.status != 200:
                print(f"åœ–ç‰‡ {img_name} è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status}")
                return
            html_content = await response.text()
            soup= BeautifulSoup(html_content, "html.parser")
            src=soup.find('img', id='img')
            if not src:
                print(f"åœ–ç‰‡ {img_name} æ²’æœ‰æ‰¾åˆ°åŸåœ–")
                return
            org_img_link= src.get('src')
            try:
                image_response = requests.get(org_img_link, headers=headers)
                if image_response.status_code == 200:
                    filepath = os.path.join(path, img_name)
                    with open(filepath, "wb") as f:
                        f.write(image_response.content)
                        print(f"å·²å„²å­˜ï¼šåœ–ç‰‡ {img_name}")
                else:
                    print(f"åœ–ç‰‡ {img_name} ä¸‹è¼‰å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status}")    
            except Exception as e:
                print(f"åœ–ç‰‡ {img_name}éŒ¯èª¤ï¼š{e}")                      
    except Exception as e:
        print(f"åœ–ç‰‡ {img_name}éŒ¯èª¤ï¼š{e}")
    return


async def download_gallery(url, start_page=1, end_page=1, auto_all=False, save_folder_path="C:\Users\weiwe\Desktop\e-hentai-images"):
    path=find_makedirs(url,save_folder_path)
    if auto_all:
        end_page = get_total_pages(url)
        start_page = 0
        print(f"ğŸ“– åµæ¸¬åˆ°ç¸½é æ•¸ï¼š{end_page}ï¼Œå°‡ä¸‹è¼‰æ•´æœ¬")
    else:
        start_page -= 1
        end_page -= 1
    async with aiohttp.ClientSession(trust_env=True) as session:
        tasks = []
        for page in range(start_page, end_page + 1):
            page_url = f"{url}?p={page}"
            tasks.append(get_image_urls_from_page(session, page_url))
        
        all_image_urls_list = await asyncio.gather(*tasks)
        all_image_urls = [item for sublist in all_image_urls_list for item in sublist]
        
        print(f"\nâœ… å…±æ‰¾åˆ° {len(all_image_urls)} å¼µåœ–ç‰‡ç¶²å€")
        
        download_tasks = [download_image(session, img_url, path) for img_url in all_image_urls]
        await asyncio.gather(*download_tasks)
        print("\nğŸ‰ æ‰€æœ‰åœ–ç‰‡ä¸‹è¼‰å®Œæˆï¼")
    return